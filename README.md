# MedEval: Evaluation Framework for Clinical LLM Outputs

MedEval is a lightweight evaluation framework designed to assess the quality of clinical outputs generated by large language models (LLMs). It supports textual output evaluation using multi-dimensional criteria like factual accuracy, coherence, relevance.

This repository contains demo notebooks to evaluate responses from two foundation models â€” **LLaMA** and **Mistral** â€” using MedEval.

## ğŸ“ Contents

- `Llama.ipynb`: Notebook to evaluate LLaMA-based responses using MedEval.
- `Mistral.ipynb`: Notebook to evaluate Mistral-based responses using MedEval.
- Sample evaluation data (embedded in notebooks or expected in external files).
- Framework functions to compute MedEval scores.

## âš™ï¸ Features

- ğŸ” **Factual Accuracy**: Verifies responses based on clinical correctness.
- ğŸ§µ **Coherence**: Checks internal consistency and logical flow.
- ğŸ¯ **Relevance**: Measures how well the response matches the question intent.
- ğŸ§ª **LLM-Aided Scoring**: Uses LLMs to judge other LLMs in a controlled rubric-guided format.

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/harini-varanasi/Med-Eval.git
cd MedEval
```

### 2. Install Dependencies

You can use `pip` or `conda` to install required packages:

```bash
pip install -r requirements.txt
```

### 3. Open and Run the Notebooks

Use Jupyter Notebook or JupyterLab:

```bash
jupyter notebook Llama.ipynb
```

or

```bash
jupyter notebook Mistral.ipynb
```

Each notebook walks you through:
- Loading the model responses
- Defining evaluation criteria
- Invoking MedEval scoring
- Displaying evaluation results with visualizations

## ğŸ“Š Sample Output

Each notebook outputs structured evaluation metrics and generated discharge summary.

## ğŸ¥ Clinical Dataset

This project uses mimic-iii data to evaluate medical reasoning. 

## ğŸ§‘â€ğŸ’» Authors

This work was developed as part of a group project by:

- **Harini Varanasi**  
- **Drashti Mehta**  
- **Akhila Madanapati**  
- **Anjali Bheemireddy**


Feel free to reach out for collaborations or questions!

## ğŸ“„ License

[MIT License](LICENSE)

---

### ğŸ“¬ Citation

If you use MedEval in your research or evaluation workflows, please cite it as:

```
@misc{MedEval2025,
  title={MedEval: Evaluation Framework for Clinical LLM Outputs},
  author={Harini Varanasi, Drashti Mehta, Akhila Madanapati, Anjali Bheemireddy
},
  year={2025},
  url={https://github.com/harini-varanasi/Med-Eval}
}
```

Please cite the following as well:

**G-Eval Evaluation Framework**  
Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C. (2023). *G-eval: NLG evaluation using gpt-4 with better human alignment.* arXiv preprint arXiv:2303.16634.
[https://doi.org/10.48550/arXiv.2303.16634]

