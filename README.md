# MedEval: Evaluation Framework for Clinical LLM Outputs

MedEval is a lightweight evaluation framework designed to assess the quality of clinical outputs generated by large language models (LLMs). It supports textual output evaluation using multi-dimensional criteria like factual accuracy, coherence, relevance.

This repository contains demo notebooks to evaluate responses from two foundation models — **LLaMA** and **Mistral** — using MedEval.

## 📁 Contents

- `Llama.ipynb`: Notebook to evaluate LLaMA-based responses using MedEval.
- `Mistral.ipynb`: Notebook to evaluate Mistral-based responses using MedEval.
- Sample evaluation data (embedded in notebooks or expected in external files).
- Framework functions to compute MedEval scores.

## ⚙️ Features

- 🔍 **Factual Accuracy**: Verifies responses based on clinical correctness.
- 🧵 **Coherence**: Checks internal consistency and logical flow.
- 🎯 **Relevance**: Measures how well the response matches the question intent.
- 🧪 **LLM-Aided Scoring**: Uses LLMs to judge other LLMs in a controlled rubric-guided format.

## 🚀 Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/harini-varanasi/Med-Eval.git
cd MedEval
```

### 2. Install Dependencies

You can use `pip` or `conda` to install required packages:

```bash
pip install -r requirements.txt
```

### 3. Open and Run the Notebooks

Use Jupyter Notebook or JupyterLab:

```bash
jupyter notebook Llama.ipynb
```

or

```bash
jupyter notebook Mistral.ipynb
```

Each notebook walks you through:
- Loading the model responses
- Defining evaluation criteria
- Invoking MedEval scoring
- Displaying evaluation results with visualizations

## 📊 Sample Output

Each notebook outputs structured evaluation metrics and generated discharge summary.

## 🏥 Clinical Dataset

This project uses mimic-iii data to evaluate medical reasoning. 

## 🧑‍💻 Authors

This work was developed as part of a group project by:

- **Harini Varanasi**  
- **Drashti Mehta**  
- **Akhila Madanapati**  
- **Anjali Bheemireddy**


Feel free to reach out for collaborations or questions!

## 📄 License

[MIT License](LICENSE)

---

### 📬 Citation

If you use MedEval in your research or evaluation workflows, please cite it as:

```
@misc{MedEval2025,
  title={MedEval: Evaluation Framework for Clinical LLM Outputs},
  author={Harini Varanasi, Drashti Mehta, Akhila Madanapati, Anjali Bheemireddy
},
  year={2025},
  url={https://github.com/harini-varanasi/Med-Eval}
}
```

Please cite the following as well:

**G-Eval Evaluation Framework**  
Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C. (2023). *G-eval: NLG evaluation using gpt-4 with better human alignment.* arXiv preprint arXiv:2303.16634.
[https://doi.org/10.48550/arXiv.2303.16634]

